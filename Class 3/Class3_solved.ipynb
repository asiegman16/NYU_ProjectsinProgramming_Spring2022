{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.3"
    },
    "colab": {
      "name": "Class3_solved.ipynb",
      "provenance": []
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z6LFPM4THY_7"
      },
      "source": [
        "# Descriptive Analysis in Python"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ohXfZh3RHY_9"
      },
      "source": [
        "Today we are going to use the NYC Vehicle Collisions '[accidents.csv](https://data.cityofnewyork.us/Public-Safety/NYPD-Motor-Vehicle-Collisions-Crashes/h9gi-nx95)' dataset.\n",
        "\n",
        "\n",
        "Remember, in order to upload a csv to Goolge Colab, you can first download it to you own machine with the following code: "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6QQyh1xDHpdf",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1f14a3a9-8252-4a0b-e324-d8653bf74317"
      },
      "source": [
        "!curl 'https://data.cityofnewyork.us/api/views/h9gi-nx95/rows.csv?accessType=DOWNLOAD' -o accidents.csv"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
            "                                 Dload  Upload   Total   Spent    Left  Speed\n",
            "100  380M    0  380M    0     0  3586k      0 --:--:--  0:01:48 --:--:-- 3902k\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hLL1lr0XHY_9"
      },
      "source": [
        "Now that we have our CSV uploaded into our Colab environment, we can 'read it in' using `pd.read_csv`"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MBldYwhTHY__"
      },
      "source": [
        "import pandas as pd\n",
        "\n",
        "df = pd.read_csv(\"./accidents.csv\",low_memory=False)\n",
        "\n",
        "# low_memory means we want to read in our full data set \n",
        "\n",
        "%matplotlib inline"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_zcz0z4xHZAC"
      },
      "source": [
        "pd.options.display.max_columns = 100"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tNYiBrgIIIkS"
      },
      "source": [
        "df.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JE73j94NHZAE"
      },
      "source": [
        "# Data Types\n",
        "\n",
        "Remember, Python is an object oriented programming language, meaning we can do different things to different objects. But how Python perceives our data (as a string, an integer, a float, a Boolean, a datetime value, et. cetera) deterimnes what we can do with our data. \n",
        "\n",
        "For instance, I can't take the average of a list of strings. So, let's use `df.dtypes` to see how Python is interpreting our data."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": true,
        "id": "NnisoEqnHZAF"
      },
      "source": [
        "df.dtypes"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aosIBfifHZAH"
      },
      "source": [
        "It looks like our date and time columns are being read in as strings, not as datetimes, so let's adjust:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W9VX85kqHZAI"
      },
      "source": [
        "df['DATETIME'] = df['CRASH DATE'] + ' ' + df['CRASH TIME'] # let's create a new column called DATETIME...\n",
        "df.DATETIME = pd.to_datetime(df.DATETIME, format=\"%m/%d/%Y %H:%M\") # ...and convert it to datetime format"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H0RaFgWsHZAK"
      },
      "source": [
        "df.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bBUfpPHqIcUQ"
      },
      "source": [
        "df.dtypes"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9Fdic8jGHZAN"
      },
      "source": [
        "We can also convert our original 'DATE' and 'TIME' columns to datetime format:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kFjMAAKafXj6"
      },
      "source": [
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ⭕ **QUESTIONS?**"
      ],
      "metadata": {
        "id": "3WpPeiGMfXj7"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X0lAImCZfXj7"
      },
      "source": [
        "---"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Yg7r8oKoHZAN"
      },
      "source": [
        "df['CRASH TIME'] = pd.to_datetime(df['CRASH TIME'], format=\"%H:%M\")"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RKHSBZRxHZAQ"
      },
      "source": [
        "df['CRASH DATE'] = pd.to_datetime(df['CRASH DATE'], format=\"%m/%d/%Y\")"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H4FrlHHhImIH"
      },
      "source": [
        "df.dtypes"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GwtMwT8LHZAS"
      },
      "source": [
        "# Engineering New Columns\n",
        "\n",
        "Now, let's create a new boolean column called \"INJURY\" and another called \"DEATH\" that only holds a True value if there was at least one injury or death in the incident:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p4aA8GDlHZAS"
      },
      "source": [
        "df['INJURY'] = (df['NUMBER OF PERSONS INJURED']>0)\n",
        "df['DEATH'] = (df['NUMBER OF PERSONS KILLED']>0)"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cHoUadNwHZAU"
      },
      "source": [
        "df.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TDYzB8ziqNNK"
      },
      "source": [
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ⭕ **QUESTIONS?**"
      ],
      "metadata": {
        "id": "bQcP_TJNqNNK"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qHOqZJw_qNNL"
      },
      "source": [
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "neap8rmgHZAX"
      },
      "source": [
        "# Exercise 1: What is the most common contributing factor to collisions?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DXuvOttWHZAX"
      },
      "source": [
        "# your code here"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5urcOdfiHZAa"
      },
      "source": [
        "# Solution"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7RpEwdIUI0Sa"
      },
      "source": [
        "df['CONTRIBUTING FACTOR VEHICLE 1'].value_counts()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WHdyxG9kHZAa"
      },
      "source": [
        "df['CONTRIBUTING FACTOR VEHICLE 1'].value_counts()[1:10].plot(kind='barh')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tFPA0JP7HZAc"
      },
      "source": [
        "# Exercise 2: Break down the number of collisions by borough"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uMhDlsDmHZAd"
      },
      "source": [
        "# your code here"
      ],
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rh6XAOoIHZAf"
      },
      "source": [
        "# Solution"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nXy-oCACI3Hp"
      },
      "source": [
        "df['BOROUGH'].value_counts()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n8w3pQljHZAf"
      },
      "source": [
        "df['BOROUGH'].value_counts().plot(kind='barh')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CdytCZ3jqWIx"
      },
      "source": [
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ⭕ **QUESTIONS?**"
      ],
      "metadata": {
        "id": "TU0ym2_8qWIx"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1J_rl7XbqWIy"
      },
      "source": [
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sqaPFhCcHZAi"
      },
      "source": [
        "# Basic Data Viz / Plotting"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Uz-8YDwVHZAi"
      },
      "source": [
        "df.plot(kind='scatter', x='LONGITUDE', y='LATITUDE')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "faWNbRr2HZAk"
      },
      "source": [
        "Immediately we see there are some erronenous values here. I don't think there were any accidents in NYC at longitude 0, latitude 40 (especially because that is somewhere in Spain, according to Google). \n",
        "\n",
        "So let's use what's known as a 'Mask' – a selection condition that only keeps the entries we determine to be valid.\n",
        "\n",
        "Another quick Google search tells us that the lat/long for NYC is between 40/41 and -72/-74.5, so let's use that."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fwJMfegGHZAl"
      },
      "source": [
        "clean_mask = (df.LATITUDE > 40) & (df.LATITUDE < 41) & (df.LONGITUDE < -72) & (df.LONGITUDE > -74.5)\n",
        "\n",
        "cleandf = df[clean_mask]"
      ],
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rDPGW4gwHZAn"
      },
      "source": [
        "cleandf.plot(kind='scatter', x='LONGITUDE', y='LATITUDE')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Pk35HsKUHZAp"
      },
      "source": [
        "Much better. Let's increase the figuresize a bit, too, just for ease of viewing. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Diq13jCfHZAp"
      },
      "source": [
        "cleandf.plot(kind='scatter', x='LONGITUDE', y='LATITUDE', figsize=(20, 15)) # increasing the figsize a bit"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N6AXpoeNqq_e"
      },
      "source": [
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ⭕ **QUESTIONS?**"
      ],
      "metadata": {
        "id": "pkMoBqPrqq_e"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fYCYMOmfqq_e"
      },
      "source": [
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H9yBhaVIHZAr"
      },
      "source": [
        "# Overplotting\n",
        "\n",
        "Although cool, this isn't exactly helpful, because there are just so many datapoints. This is known as \"Overplotting.\"\n",
        "\n",
        "To get around overplotting, we can sample our data by either specifying the number of points we want to keep, or the percentage of our data set that we want to keep."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g8NQRIe4HZAr"
      },
      "source": [
        "## `sample(n= ...)` \n",
        "\n",
        "let's us keep a specified number of data points to plot"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T5l8tmNgHZAs"
      },
      "source": [
        "sample = cleandf.sample(n=10000) # keep 10,000 data points\n",
        "\n",
        "sample.plot(kind='scatter', x='LONGITUDE', y='LATITUDE', figsize=(20, 15))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QWSox4WCHZAu"
      },
      "source": [
        "## `sample(frac= ...)` \n",
        "\n",
        "let's us keep a specified percentage (fraction) of data points to plot"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iHahFm06HZAu"
      },
      "source": [
        "sample = cleandf.sample(frac=0.01) # keep 1% of the dataset\n",
        "\n",
        "sample.plot(kind='scatter', x='LONGITUDE', y='LATITUDE', figsize=(20, 15))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tl6MEBBuHZAw"
      },
      "source": [
        "## `s`\n",
        "\n",
        "let's us change the marker size"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KmRvnujaHZAw"
      },
      "source": [
        "cleandf.plot(\n",
        "    kind='scatter', x='LONGITUDE', y='LATITUDE', figsize=(20, 15), s=0.5 ) # changing the marker size ('s')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uywjToFpHZA0"
      },
      "source": [
        "## `alpha`\n",
        "\n",
        "let's us change the transparency of the marker"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ihreVpXNHZA1"
      },
      "source": [
        "cleandf.plot(\n",
        "    kind='scatter',\n",
        "    x='LONGITUDE',\n",
        "    y='LATITUDE',\n",
        "    figsize=(20, 15),\n",
        "    s=0.5,\n",
        "    alpha=0.05) # changing the marker transparency ('alpha')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OvxEVK1PqubA"
      },
      "source": [
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ⭕ **QUESTIONS?**"
      ],
      "metadata": {
        "id": "_AmvM59oqubB"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MWku4gX3qubJ"
      },
      "source": [
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IoW1mBulHZA4"
      },
      "source": [
        "# Datetimes...Again\n",
        "\n",
        "Now, back to working with datetimes, in particular, time series data (data that is obtained at successive times, often with equal intervals between them).\n",
        "\n",
        "Pandas has proven very successful as a tool for working with time series data, especially in the financial data analysis space. Using the [NumPy](http://www.numpy.org/) `datetime64` and `timedelta64` dtypes, pandas consolidated a large number of features from other Python libraries as well as created a tremendous amount of new functionality for manipulating time series data.\n",
        "\n",
        "To illustrate, let's generate a list of datetime values for 3 neighboring days starting from `01/30/2017` with the interval equals to one hour. \n",
        "\n",
        "The [`date_range`](http://pandas.pydata.org/pandas-docs/version/0.19.1/generated/pandas.date_range.html) function can return such list in [`DatetimeIndex`](http://pandas.pydata.org/pandas-docs/stable/generated/pandas.DatetimeIndex.html) format. \n",
        "\n",
        "We simply should define the left bound for generating dates `start` (`\"01/30/2017\"` in our case), the right bound for generating dates `end` or the amount of intervals `periods` (`3*24` in our case, because we are going to cover three days) and preferable frequency `freq` (`'H'` in our case)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D3X7BgnNHZA4"
      },
      "source": [
        "example_range = pd.date_range(start='01/30/2017', periods=3*24, freq='H')\n",
        "\n",
        "print(\"Number of elements:\", len(example_range.values))\n",
        "example_range[:10]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "25zc3KNfHZA6"
      },
      "source": [
        "## `pd.to_datetime`"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZsYosVQ8HZA6"
      },
      "source": [
        "To convert a `Series` or list-like object of date-like objects e.g. strings, epochs, or a mixture, you can use the [`to_datetime`](http://pandas.pydata.org/pandas-docs/version/0.19.2/generated/pandas.to_datetime.html) function. When passed a `Series`, this returns a Series (with the same index), while a list-like is converted to a `DatetimeIndex`:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jVqdNKe9HZA7"
      },
      "source": [
        "pd.to_datetime(pd.Series(['Jul 31, 2009', '2010-12-10', None])) "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SAiJ1mT6HZA9"
      },
      "source": [
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XH5rz2hUHZA9"
      },
      "source": [
        "# New Data!\n",
        "\n",
        "To better illustrate working with datetimes, let's look at the [Daily minimum temperatures in Melbourne data set from Kaggle](https://www.kaggle.com/paulbrabban/daily-minimum-temperatures-in-melbourne) \n",
        "\n",
        "You can download this dataset from Brightspace and upload it into your Colab environment. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": true,
        "id": "iKF8ZtSOHZA-"
      },
      "source": [
        "melbourne_temp = pd.read_csv(\"./daily-minimum-temperatures-in-melbourne.csv\", \n",
        "                             skiprows=1,  # let's us skip the header\n",
        "                             names=[\"date\", \"temp\"])  # let's name columns as we desire\n",
        "melbourne_temp"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CpgZu-eSHZA_"
      },
      "source": [
        "Let's drop that final row (the one that appears to just be a description of the data set)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CMeYCQaxHZBA"
      },
      "source": [
        "melbourne_temp.drop(melbourne_temp.tail(1).index,inplace=True) \n",
        "\n",
        "melbourne_temp.tail()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lz7urPXeHZBC"
      },
      "source": [
        "melbourne_temp.dtypes"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k8tzi5mIHZBD"
      },
      "source": [
        "## `to_numeric` \n",
        "\n",
        "Along with converting our 'date' column to a datetime dtype, we can also use `pd.to_numeric` to convert the temperatures in our data set to a numeric type. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pVH3z8_8HZBE"
      },
      "source": [
        "melbourne_temp['temp'] = melbourne_temp['temp'].apply(pd.to_numeric, errors=\"coerce\")\n",
        "melbourne_temp['date'] = pd.to_datetime(melbourne_temp['date'])"
      ],
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ni5-WnhiHZBF"
      },
      "source": [
        "melbourne_temp.dtypes"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xDV2xkCJHZBH"
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "from matplotlib.pylab import rcParams\n",
        "\n",
        "rcParams['figure.figsize'] = (16, 6) # change figsize \n",
        "\n",
        "melbourne_temp.set_index('date', inplace=True)"
      ],
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "px1QKgs-HZBJ"
      },
      "source": [
        "melbourne_temp.plot()\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0WkATaVjHZBK"
      },
      "source": [
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ⭕ **QUESTIONS?**"
      ],
      "metadata": {
        "id": "KV8q8ylgq41u"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c64sUvGUq41v"
      },
      "source": [
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "THf6zc5GHZBL"
      },
      "source": [
        "# Exercise 1: Find the minimum temperture for all of the recorded history included in this data set:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dnxppTV2HZBL"
      },
      "source": [
        "# your code here"
      ],
      "execution_count": 39,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WNW2YOXdHZBN"
      },
      "source": [
        "# Solution"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Vx4NIpRnHZBO"
      },
      "source": [
        "melbourne_temp.min()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AUGhi8d2HZBP"
      },
      "source": [
        "# Exercise 2: Find the temperature for every day in January, 1981:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MzT6y7O0HZBQ"
      },
      "source": [
        "# your code here"
      ],
      "execution_count": 41,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yNc-xfNQHZBR"
      },
      "source": [
        "# Solution"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": true,
        "id": "EhYVCW6dHZBR"
      },
      "source": [
        "melbourne_temp['1981-01']"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xWjJCyTCHZBU"
      },
      "source": [
        "# Exercise 3: Find the temperature for every day between Jan. 5, 1990 and Jan. 12, 1990:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xXRG3fmcHZBU"
      },
      "source": [
        "# your code here"
      ],
      "execution_count": 43,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I3KT_EfxHZBX"
      },
      "source": [
        "# Solution"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BcZDS2s7HZBX"
      },
      "source": [
        "melbourne_temp['1990-01-05':'1990-01-12']"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "maCITJtDHZBZ"
      },
      "source": [
        "# Exercise 4: Plot the temperature for every day in February, 1981:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VoILVMQ4HZBa"
      },
      "source": [
        "# your code here"
      ],
      "execution_count": 45,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WGy1ppwFHZBc"
      },
      "source": [
        "# Solution"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uBaD3narHZBc"
      },
      "source": [
        "melbourne_temp['1981-02'].plot()\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e1inMNgHHZBf"
      },
      "source": [
        "# Exercise 5: Find the average temperature in February, 1981:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4W3Z53z8HZBf"
      },
      "source": [
        "# your code here"
      ],
      "execution_count": 47,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-uUYHQu4HZBh"
      },
      "source": [
        "# Solution"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ScUJ_78NHZBh"
      },
      "source": [
        "melbourne_temp['1981-02'].mean()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eNN1d0YuHZBj"
      },
      "source": [
        "# Exercise 6: Find how many days the temperature was less than 2 degress across the whole data set:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1JxaFQvuHZBj"
      },
      "source": [
        "# your code here"
      ],
      "execution_count": 49,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E5Sp4FLiHZBl"
      },
      "source": [
        "# Solution"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sTz6viVgHZBm"
      },
      "source": [
        "print(\"Days with temperature less than 2 degrees:\", (melbourne_temp < 2).sum().values[0])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KbkKrjQerckK"
      },
      "source": [
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ⭕ **QUESTIONS?**"
      ],
      "metadata": {
        "id": "Sggb0_9ArckK"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "syvAC7zvrckS"
      },
      "source": [
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hGDOFEUEHZBo"
      },
      "source": [
        "# Resampling\n",
        "\n",
        "Resampling involves changing the frequency of your time series observations. \n",
        "\n",
        "- Upsampling means you increase the frequency of the samples (say, from minutes to seconds)\n",
        "- Downsampling means you decrease the frequency of the samples (say, from months to days)\n",
        "\n",
        "Imagine we are trying to determine the average weekly temperture from our Melbourne set, for instance. In that case, we would need to upsample the information from daily to weekly. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BTw8HrRSHZBo"
      },
      "source": [
        "melbourne_temp.resample('W').mean().head(10)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0ovi3x3xHZBp"
      },
      "source": [
        "melbourne_temp.resample('A').min() # 'A' means year-end frequency"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-rpQ5sr7HZBs"
      },
      "source": [
        "Resampling is also a fast way to smooth in some essence the time series. For instance, the time series of monthly averages has much less peaks or outliers and allows tracking the periodicity. \n",
        "\n",
        "One way to smooth a time series is to calculate a rolling average – the average value of the current point and `N` previous points. It can be calcualted using [`rolling`](http://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.rolling.html) pandas's method."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2gKnfMxTHZBs"
      },
      "source": [
        "ax = melbourne_temp.plot(alpha=0.25) # draw initial time series and make it transparent\n",
        "\n",
        "melbourne_temp.resample('M').mean().plot(ax=ax) # draw montly average values\n",
        "\n",
        "melbourne_temp.rolling(25).mean().plot(ax=ax) # draw roling average that takes into account 25 points\n",
        "\n",
        "ax.legend([\"daily data\", \"montly average\", \"roling mean\"])\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KYvzY5QeHZBw"
      },
      "source": [
        "If you set the hourly frequency in the [`resample()`](http://pandas.pydata.org/pandas-docs/version/0.17.0/generated/pandas.DataFrame.resample.html) function, then `NaN` values will be created, because there are no more tiny distribution of the temperature.\n",
        "\n",
        "This is upsampling."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": true,
        "id": "V1ZGJWPyHZBw"
      },
      "source": [
        "upsampled = melbourne_temp.resample('H').mean()\n",
        "upsampled.head(30)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "73dzLQllHZBx"
      },
      "source": [
        "But we can interpolate the missing values at this new frequency.\n",
        "\n",
        "The `Series` pandas's object provides the [`interpolate`](http://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.interpolate.html) function to interpolate missing values. A good starting point is to use a `linear interpolation`. This draws a straight line between available data and fills in values at the chosen frequency from this line."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": true,
        "id": "qyI7o4UhHZBx"
      },
      "source": [
        "interpolated = upsampled.interpolate(method='linear')\n",
        "interpolated.head(30)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_P9Cmj63rd8P"
      },
      "source": [
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ⭕ **QUESTIONS?**"
      ],
      "metadata": {
        "id": "TIcHVGZqrd8P"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ge8q9lZMrd8P"
      },
      "source": [
        "---"
      ]
    }
  ]
}